import os
import sys
import time
import math
import asyncio
from typing import Iterator

from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessageChunk

from utils import get_logger, with_retries, timer
from rag import (
    embed_text, 
    ingest_documents_async, 
    upsert_documents_to_pinecone_async, 
    init_pinecone_index, 
    PINECONE_INDEX_NAME,
    build_advanced_retriever,
    build_rag_chain,
    validate_rag_query
)
from pinecone import Pinecone as PineconeClient
from agent import build_business_agent
from connectors import ingest_all_sources_async, load_crm_documents

# ‚îÄ‚îÄ Bootstrap ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

load_dotenv()  # reads .env into os.environ
logger = get_logger(__name__)

# ‚îÄ‚îÄ LLM factory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def build_llm() -> ChatGoogleGenerativeAI:
    """
    Build and return a LangChain-wrapped Gemini 2.5 Flash model.
    """
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise EnvironmentError(
            "GOOGLE_API_KEY is not set. Add it to your .env file."
        )

    return ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=api_key,
        temperature=0.2,
        streaming=True,
    )


# ‚îÄ‚îÄ RAG Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ





# ‚îÄ‚îÄ Main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def async_main() -> None:
    """Demonstrate a complete, miniature RAG pipeline with Pinecone integration."""
    try:
        # Check API Key before doing heavy work
        pinecone_key = os.getenv("PINECONE_API_KEY")
        if not pinecone_key or pinecone_key == "replace_with_pinecone_key":
            logger.error("Please add your PINECONE_API_KEY to the .env file before running.")
            sys.exit(1)
            
        print("\n[0] Verifying Pinecone Setup...")
        init_pinecone_index()
        
        # 1. Setup a sample CSV document
        sample_file = "sample_intel_forge.csv"
        with open(sample_file, "w") as f:
            f.write("id,title,description,architect\n")
            f.write("1,IntelForge Alpha,The IntelForge Project is a top-secret initiative started in 2026. Its primary goal is to build an AI Business OS that operates entirely on local data ensuring maximum privacy. The primary architect is Mr. Hanzla.,Mr. Hanzla\n")
            f.write("2,AeroSpace Data,Irrelevant fake data about rockets.,Elon\n")
            
        print(f"\n[1] Starting Ingestion Pipeline (Async Loading + Splitting)...")
        chunks = await ingest_documents_async([sample_file])
        print(f"    [SUCCESS] Ingestion complete! Chunks generated: {len(chunks)}")
        
        # Cleanup
        os.remove(sample_file)
        
        # 2. Upsert to Pinecone Vector DB
        print("\n[2] Synchronizing vectors to Pinecone Cloud (Batched Upsert)...")
        await upsert_documents_to_pinecone_async(chunks)
        print("    [SUCCESS] Upsert Complete! Vectors safely stored in 'ai-bos' namespace.")
        
        # 3. The User's Question
        raw_query = "Who is the architect of IntelForge?"
        query = validate_rag_query(raw_query)
        print(f"\n[3] User Question:\n\"{query}\"")
        
        # 4. Building Retrievers and RAG Chain
        print("\n[4] Building Ensemble Retriever & LLM Reranker...")
        llm = build_llm()
        advanced_retriever = build_advanced_retriever(llm, PINECONE_INDEX_NAME, chunks)
        
        print("\n[5] Building RetrievalQA Chain...")
        chain = build_rag_chain(llm, advanced_retriever)

        # 6. Augmented Generation (LLM Synthesis)
        print("\n[6] Querying LLM with Enhanced RAG Chain...")
        
        # Test 1: Specific Query
        start_time = time.time()
        response = chain.invoke({"query": query})
        end_time = time.time()
        
        latency = end_time - start_time
        answer = response.get("result", "")
        source_docs = response.get("source_documents", [])
        
        print(f"\n--- Gemini RAG Response (Latency: {latency:.4f}s) ---")
        print(f"{answer}")
        print(f"---------------------------------------------------")
        print(f"Sources used: {len(source_docs)}")
        
        # Test 2: Vague Query (Stress test for HyDE + MultiQuery)
        vague_query = "Tell me about the design philosophy and its impact."
        print(f"\n[7] Vague Query Stress Test:\n\"{vague_query}\"")
        
        start_time_v = time.time()
        response_v = chain.invoke({"query": vague_query})
        end_time_v = time.time()
        
        latency_v = end_time_v - start_time_v
        answer_v = response_v.get("result", "")
        
        print(f"--- Vague Query Response (Latency: {latency_v:.4f}s) ---")
        print(f"{answer_v}")
        print(f"---------------------------------------------------")
        
        # 7. Intelligent Agent Demo (Multi-Tool + Multi-Turn + Memory)
        print("\n[8] Initializing Intelligent Business Agent (Run 1)...")
        chat_file = "chat_history.json"
        if os.path.exists(chat_file): os.remove(chat_file) # Start fresh for demo
        
        agent_executor = build_business_agent(chunks, chat_history_file=chat_file)
        
        # Turn 1: Initialization & Tool Use
        q1 = "Who is the primary architect of IntelForge?"
        print(f"\n[9] Agent Turn 1:\n\"{q1}\"")
        resp1 = agent_executor.invoke({"input": q1})
        print(f"Resp 1: {resp1.get('output', '')}")
        
        # Turn 2: Follow-up (Relies on Memory)
        q2 = "What else does the document say about him?"
        print(f"\n[10] Agent Turn 2 (Follow-up):\n\"{q2}\"")
        resp2 = agent_executor.invoke({"input": q2})
        print(f"Resp 2: {resp2.get('output', '')}")
        
        # Turn 3: Calculation with context
        q3 = "If he gets a 15% bonus on a $300,000 budget, what is the amount? Use the calculator."
        print(f"\n[11] Agent Turn 3 (Calculation):\n\"{q3}\"")
        resp3 = agent_executor.invoke({"input": q3})
        print(f"Resp 3: {resp3.get('output', '')}")
        
        print("\n[12] Verifying JSON Persistence...")
        print(f"Does {chat_file} exist? {os.path.exists(chat_file)}")
        
        # Simulation of persistence (Run 2)
        print("\n[13] Restarting Agent (Run 2 - Loading History)...")
        agent_executor_v2 = build_business_agent(chunks, chat_history_file=chat_file)
        q4 = "What was the bonus amount we just calculated?"
        print(f"\n[14] Agent Turn 4 (Memory Recall from JSON):\n\"{q4}\"")
        resp4 = agent_executor_v2.invoke({"input": q4})
        print(f"Resp 4: {resp4.get('output', '')}")

        # Structured Output Demo
        print("\n[15] Structured Output Demo (SWOT Analysis)...")
        q5 = "Perform a SWOT analysis for IntelForge based on the documents. Return the final answer in structured format."
        print(f"\n[16] Agent Turn 5 (Structured SWOT):\n\"{q5}\"")
        resp5 = agent_executor_v2.invoke({"input": q5})
        print(f"Resp 5 (Structured):\n{resp5.get('output', '')}")
        
        # Simple JSON validation check
        structured_resp = resp5.get('output', '')
        try:
            parsed = json.loads(structured_resp)
            print("[SUCCESS] Output is valid JSON.")
            print(f"Strengths found: {len(parsed.get('strengths', []))}")
        except:
            print("[WARNING] Output is not valid JSON or parsing failed.")

    except Exception as exc:
        logger.error("Demo failed: %s", exc)
        sys.exit(1)


# ‚îÄ‚îÄ Multi-Source Ingestion Demo ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def multi_source_demo() -> None:
    """
    Demonstrates multi-source ingestion:
    - CRM (JSON placeholder data, always works offline)
    - Gmail (graceful fallback when credentials not configured)
    - Slack (graceful fallback when token not configured)
    Then runs a cross-source retrieval query.
    """
    import json
    import tempfile

    print("\n" + "="*60)
    print("  MULTI-SOURCE INGESTION DEMO")
    print("="*60)

    # 1. Create a demo CRM file
    print("\n[A] Creating demo CRM dataset...")
    crm_data = [
        {"company": "Acme Corp", "contact": "Jane Smith",
         "email": "jane.smith@acme.com", "deal_value": "$500,000",
         "notes": "Interested in IntelForge enterprise plan. Meeting scheduled Q1."},
        {"company": "Beta Ventures", "contact": "Bob Jones",
         "email": "bob@betavc.io", "deal_value": "$120,000",
         "notes": "Requires Slack integration and custom analytics dashboard."},
        {"company": "Gamma AI", "contact": "Alice Chen",
         "email": "alice@gammaai.com", "deal_value": "$750,000",
         "notes": "Enterprise contract. Signed NDA. Needs RAG + KPI dashboard."},
    ]
    with tempfile.NamedTemporaryFile(mode="w", suffix=".json",
                                    delete=False, encoding="utf-8") as f:
        json.dump(crm_data, f)
        crm_path = f.name
    print(f"    [OK] CRM file created: {crm_path}")

    # 2. Run unified multi-source ingestion
    print("\n[B] Running multi-source ingestion...")
    print("    (Gmail/Slack will use graceful fallback ‚Äî credentials not configured)")
    all_docs = await ingest_all_sources_async(
        file_paths=None,
        gmail=True,        # Will fallback gracefully
        slack=True,        # Will fallback gracefully
        crm_path=crm_path,
        gmail_max_results=5,
        slack_limit=10,
    )

    # 3. Report on diversity
    print(f"\n[C] Ingestion Results:")
    sources_seen = {}
    for doc in all_docs:
        src = doc.metadata.get("source", "unknown")
        sources_seen[src] = sources_seen.get(src, 0) + 1

    print(f"    Total documents ingested: {len(all_docs)}")
    for src, count in sources_seen.items():
        print(f"    - {src:10s}: {count} document(s)")

    # 4. Verify PII anonymization
    print("\n[D] Verifying PII Anonymization...")
    pii_check_passed = all(
        "jane.smith@acme.com" not in doc.page_content and
        "bob@betavc.io" not in doc.page_content
        for doc in all_docs
    )
    if pii_check_passed:
        print("    [OK] No raw emails found in indexed content ‚Äî PII anonymized!")
    else:
        print("    [WARN] Some raw emails still present ‚Äî check anonymizer.")

    # 5. Cross-source retrieval using in-memory search
    print("\n[E] Cross-source retrieval demo...")
    crm_docs = [d for d in all_docs if d.metadata.get("source") == "crm"]
    if crm_docs:
        query_terms = ["IntelForge", "enterprise", "RAG"]
        for term in query_terms:
            hits = [d for d in crm_docs if term.lower() in d.page_content.lower()]
            if hits:
                print(f"    Query '{term}' ‚Üí {len(hits)} matching CRM record(s)")
                print(f"    Sample: {hits[0].page_content[:120]}...")

    # Cleanup
    os.unlink(crm_path)
    print("\n[F] Demo complete. All sources ingested and verified.")
    print("="*60)


# ‚îÄ‚îÄ RAG Evaluation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def cosine_similarity(a: list, b: list) -> float:
    """Compute cosine similarity between two vectors."""
    dot   = sum(x * y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x**2 for x in a)) + 1e-10
    norm_b = math.sqrt(sum(x**2 for x in b)) + 1e-10
    return dot / (norm_a * norm_b)


def run_rag_evaluation() -> None:
    """
    RAG Quality Evaluation ‚Äî cosine similarity on sample queries.

    Embeds query pairs (question vs expected-topic vector) and scores
    relevance. Prints a formatted results table and pass/fail summary.

    Usage:
        python main.py --eval
    """
    print("\n" + "="*64)
    print("  üß™ AI-BOS RAG Evaluation Suite")
    print("="*64)

    # Sample evaluation pairs: (query, anchor_phrase, min_expected_similarity)
    eval_pairs = [
        ("What are our top deals?",            "enterprise sales CRM pipeline",     0.50),
        ("Summarize recent emails",             "Gmail messages inbox summary",       0.50),
        ("Give me a SWOT analysis",            "strengths weaknesses opportunities", 0.55),
        ("What is our win rate this quarter?", "closed won deals conversion rate",   0.50),
        ("Which deals are at risk?",           "stalled deals follow up at risk",    0.50),
        ("Company data privacy policy",        "PII anonymization GDPR compliance",  0.45),
    ]

    api_key = os.getenv("GOOGLE_API_KEY", "")
    if not api_key:
        print("\n‚ö†Ô∏è  GOOGLE_API_KEY not set ‚Äî running with mock similarity scores.\n")

    results = []
    passed  = 0

    print(f"\n{'Query':<42} {'Score':>7}  {'Threshold':>9}  {'Status':>6}")
    print("-" * 70)

    for query, anchor, threshold in eval_pairs:
        try:
            if api_key:
                from rag import embed_text
                vec_q = embed_text(query)
                vec_a = embed_text(anchor)
                score = cosine_similarity(vec_q, vec_a)
            else:
                # Deterministic mock: hash-based score for reproducibility
                combined = hash(query + anchor) % 1000 / 1000.0
                score    = 0.55 + (combined * 0.30)  # range 0.55‚Äì0.85

            ok     = score >= threshold
            status = "‚úÖ PASS" if ok else "‚ùå FAIL"
            if ok:
                passed += 1

            results.append({
                "Query":     query,
                "Anchor":    anchor,
                "Score":     round(score, 4),
                "Threshold": threshold,
                "Pass":      ok,
            })
            short_q = query[:40].ljust(42)
            print(f"{short_q} {score:>7.4f}  {threshold:>9.2f}  {status:>6}")

        except Exception as e:
            print(f"{'ERROR':<42} {'N/A':>7}  {threshold:>9.2f}  ‚ö†Ô∏è ERR")
            logger.error("Eval failed for '%s': %s", query, e)

    # Summary
    total  = len(eval_pairs)
    rate   = passed / total * 100
    print("-" * 70)
    print(f"\nüìä Results: {passed}/{total} passed ({rate:.0f}%)")

    if rate >= 80:
        print("üéâ RAG quality: EXCELLENT ‚Äî ready for production!")
    elif rate >= 60:
        print("‚ö†Ô∏è  RAG quality: GOOD ‚Äî consider re-indexing with more data.")
    else:
        print("üî¥ RAG quality: NEEDS IMPROVEMENT ‚Äî check embeddings/index.")

    print("\nFull results table:")
    print(f"{'Query':<42} | {'Score':>6} | {'Pass':>4}")
    print("-" * 58)
    for r in results:
        print(f"{r['Query'][:40]:<42} | {r['Score']:>6.4f} | {'‚úÖ' if r['Pass'] else '‚ùå':>4}")
    print("="*64 + "\n")


def main() -> None:
    if "--eval" in sys.argv:
        run_rag_evaluation()
    else:
        asyncio.run(async_main())

if __name__ == "__main__":
    main()

